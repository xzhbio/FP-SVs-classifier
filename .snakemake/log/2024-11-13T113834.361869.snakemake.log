Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 52
Rules claiming more threads will be scaled down.
Job stats:
job            count
-----------  -------
all                1
model_check        1
total              2

Select jobs to execute...

[Wed Nov 13 11:38:39 2024]
rule model_check:
    input: test/hairpin.npy
    output: test/FP-SVs_reads_id.txt
    log: test/model_check.log
    jobid: 2
    reason: Missing output files: test/FP-SVs_reads_id.txt
    wildcards: out_dir=test
    resources: tmpdir=/tmp

[Wed Nov 13 11:39:09 2024]
Error in rule model_check:
    jobid: 2
    input: test/hairpin.npy
    output: test/FP-SVs_reads_id.txt
    log: test/model_check.log (check log file(s) for error details)
    shell:
        python script/signal_classify_ResNet.py --load pretrain_model/ResNet_micro_all.pth --external test/hairpin.npy --output test/FP-SVs_reads_id.txt --cuda 0
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job model_check since they might be corrupted:
test/FP-SVs_reads_id.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-11-13T113834.361869.snakemake.log
